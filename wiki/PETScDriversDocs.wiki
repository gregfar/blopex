#summary Documentation needed for our PETSc test drivers

= Introduction =

Our PETSc test drivers are very poorly documented. 
To such an extreme that it is in some cases impossible to conclude from their 
results if they actually work. 
Let us provide the description of the drivers here, to be added to the headers 
of the c codes and/or readme files later.  


== *driver* ==

driver builds a 7pt laplacian for solution and calls either lobpcg_solve_complex if Petsc is configured for complex (this is controlled by the PETSC_USE_COMPLEX preprocessor variable) or lobpcg_solve_double if Petsc is configured for real (double).
{{{
Usage: mpirun -np <procs> ./driver [-help] [all PETSc options]

Special options:
-n_eigs <integer>      Number of eigenvalues to calculate
-tol <real number>     absolute tolerance for residuals
-full_out              Produce more output
-freepart              Let PETSC fully determine partitioning
-seed <integer>        seed for random number generator
-itr <integer>         Maximal number of iterations

Example:
mpirun -np 2 ./driver -n_eigs 3 -tol 1e-6 -itr 20
}}}

== *driver_diag* ==

driver_diag solves an eigenvalue problem for a diagonal matrix.  The diagonal matrix is generated within the program with size (rows) controlled by parameter "-N xxxx" with default of "xxxx=100".  Diagonal entries are 1 through xxxx.

{{{
Usage: mpirun -np <procs> driver_fiedler [-help] [all PETSc options]

Special options:
-n_eigs <integer>      Number of eigenvalues to calculate
-tol <real number>     absolute tolerance for residuals
-full_out              Produce more output
-seed <integer>        seed for random number generator
-itr <integer>         Maximal number of iterations
-output_file <string>  Filename to write calculated eigenvectors.
-shift <real number>   Apply shift to 'stiffness' matrix
-N <integer>           Size of Matrix to create 

Example:
mpirun -np 2 ./driver_diag -N 10000 -n_eigs 3 -tol 1e-6 -itr 20";
}}}

== *driver_fiedler* ==

driver_fiedler accepts as input matrices in Petsc format. These matrices must be setup for either 32bit or 64bit arithematic, real or complex. The test matrices are setup for double (real) or complex and for 32bit or 64bit. The 64bit version have a file name containing "64". All our matrices for testing can be downloaded from http://code.google.com/p/blopex/downloads/detail?name=blopex_petsc_test.tar.gz

driver_fiedler accepts as input the matrix A in Petsc format. These can be setup via some Matlab programs in the PETSc socket interface to Matlab; PetscBinaryRead.m and PetscBinaryWrite.m. These programs read and write Matlab matrices and vectors to files formatted for Petsc. The version from Petsc only supports double. We have modified these programs to also support complex and 64bit integers. Our versions are included in the Google source directory  ../blopex_petsc along with PetscWriteReadExample.m to illustrate how to use them.

{{{
Usage: mpirun -np <procs> ./driver_fiedler [-help] [all PETSc options]

Special options:
-matrix <filename>      (mandatory) specify file with 'stiffness' matrix (in petsc format)
-mass_matrix <filename> (optional) 'mass' matrix for generalized eigenproblem
-n_eigs <integer>      Number of eigenvalues to calculate
-tol <real number>     absolute tolerance for residuals
-full_out              Produce more output
-seed <integer>        seed for random number generator
-itr <integer>         Maximal number of iterations
-output_file <string>  Filename to write calculated eigenvectors.
-shift <real number>   Apply shift to 'stiffness' matrix 

Example:
mpirun -np 2 ./driver_fiedler -matrix my_matrix.bin -n_eigs 3 -tol 1e-6 -itr 20";
}}}

== *driver_fortran* == 

is an example of a PETSc call (BLOPEX call in this case) from FORTRAN 90. This is a modified version of the Fortran example ex2f.F from the PETSc source distribution (Version 2.3.3-p4). It calls the LOBPCG solver from BLOPEX with blocksize=5 and displays the final eigenvalues and the corresponding residuals. There is no history output. The executable *ex2f_blopex* not accept any command-line options. 

*ex2f_blopex* solves a standard eigenvalue problem for a matrix A, which is the negative 2D Laplacian approximated on the standard 5-point stencil. Technically, *ex2f_blopex* solves a generalized eigenvalue problem, but the second, "mass," matrix is just an identity. 

The domain is a square, the mesh is 30-by-30, the mesh step is one, the BC are Dirichlet.  
The exact eigenvalues using http://www.mathworks.com/matlabcentral/fileexchange/27279-laplacian-in-1d-2d-or-3d specifically calling   
{{{
>> [A,lambda]=laplacian([30,30],5); lambda
}}}
are
{{{
     2.052270643241941e-02
     5.120147071122071e-02
     5.120147071122071e-02
     8.188023499002201e-02
     1.019828404161120e-01
}}}

The default KSP preconditioning is used for the matrix A. The default KSP type is GMRES with a restart of 30, using modified Gram-Schmidt orthogonalization, see KSPCreate PETSc docs. 

Here is an example of a typical one-CPU run:  

{{{
./ex2f_blopex
BLOPEX complete in  12 iterations
eval   1 is 0.205227064E-01 residual 0.1767E-07
eval   2 is 0.512014707E-01 residual 0.4771E-07
eval   3 is 0.512014707E-01 residual 0.2177E-06
eval   4 is 0.818802350E-01 residual 0.8697E-06
eval   5 is 0.101982840E+00 residual 0.4592E-06 
}}}

Here is an example of a typical 4-CPU run:  
{{{
mpirun -np 4 ./ex2f_blopex
BLOPEX complete in  13 iterations
eval   1 is 0.205227064E-01 residual 0.2477E-07
eval   2 is 0.512014707E-01 residual 0.1560E-06
eval   3 is 0.512014707E-01 residual 0.2785E-06
eval   4 is 0.818802350E-01 residual 0.7383E-06
eval   5 is 0.101982840E+00 residual 0.9081E-06
}}}

The residuals will vary with the number of CPUs and on different computers.  

*ex2f_blopex* computes 5 eigenpairs simultaneously in a block and displays only the final results. To see the complete convergence history, edit the code *blopex_c.c* and replace 0 with 2 in lines 241 and 262. 

*BLOPEX 1.1 changes in in blopex_c.c* :

 # Add checks for PETSC_USE_COMPLEX to call correct function for lobpcg_solve
 # Add type casting when moving void parameter points
 # Surround petsc_lobpcg_solve_c_ with  extern "C" { .... }. This is to enforce compatibility of object names between fortran and C++.

*Possible improvement.* 

 # Check the most recent version of the Fortran example ex2f.F and the related C code from the PETSc source and try to sync with them. 
 # What does the code actually do in complex arithmetic? 