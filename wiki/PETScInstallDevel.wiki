#summary Installing and testing the development versions of PETSc and SLEPc.

= Introduction =

BLOPEX is currently being in the process of move from PETSc to SLEPc. The SLEPc BLOPEX install needs to be tested. 

Currently, the development versions of PETSc and SLEPc need to be used to test the latest BLOPEX in SLEPc. 

= Installing the development versions of PETSc (without BLOPEX) =

Follow http://www.mcs.anl.gov/petsc/petsc-as/developers/index.html#Obtaining

For configure, use the usual options that need to be tested. DO NOT install BLOPEX with PETSc. After configure, compile as usual. During the PETSc installation, you must have set up both PETSC_DIR and PETSC_ARCH enviroment variables. They are needed in the next step, the SLEPc installation. 

= Installing the development versions of SLEPc with BLOPEX =

According to http://www.grycap.upv.es/slepc/download/download.htm, run
{{{
svn checkout http://www.grycap.upv.es/slepc/svn/trunk slepc-dev
}}}
e.g., in the same directory where the newly created petsc-dev directory is located. At a later time, the repository can be refreshed simply by:
{{{
svn update
}}}

As suggested at http://www.grycap.upv.es/slepc/documentation/instal.htm execute 
{{{
cd slepc-dev/
export SLEPC_DIR=$PWD
./configure --download-blopex 
make
make test
}}}

The SLEPc configure reads PETSc configure parameters using PETSC_DIR, so PETSc configure must be executed first, as in the previous step. 

= Running SLEPc examples with BLOPEX =

BLOPEX-compatible SLEPc examples are listed at 
http://www.grycap.upv.es/slepc/documentation/current/src/examples/index.html
and are located in src/eps/examples/tutorials . BLOPEX-compatible SLEPc examples are 1-4, 7, 11, and 19. To use BLOPEX in any BLOPEX-compatible SLEPc example, simply add "-eps_type blopex" to the command line. In order to call hypre preconditioning, add also -"st_pc_type hypre -st_pc_hypre_type boomeramg" assuming that PETSc has been configured with hypre. The option "-st_ksp_type preonly" is the default in the following:


{{{
./ex19 -da_grid_x 100 -da_grid_y 100 -da_grid_z 100 -eps_type blopex -eps_nev 2 -eps_tol 1e-11 -eps_max_it 10 -st_pc_type hypre -st_pc_hypre_type boomeramg  -showtimes -eps_view
}}}

For large problems, the code may run out of memory and crash. In this case, add the following extra options 
{{{
-st_pc_hypre_boomeramg_agg_nl 1 -st_pc_hypre_boomeramg_P_max 4 -st_pc_hypre_boomeramg_interp_type standard 
}}}

so that you run 

{{{
./ex19 -eps_type blopex -st_pc_type hypre -st_pc_hypre_type boomeramg -st_pc_hypre_boomeramg_agg_nl 1 -st_pc_hypre_boomeramg_P_max 4 -st_pc_hypre_boomeramg_interp_type standard -showtimes -eps_view
}}}


In the following example, the preconditioner for the BLOPEX eigensolver is the PCG iterative method, preconditioned with hypre's boomeramg:

{{{
./ex19 -eps_type blopex -st_ksp_type cg -st_pc_type hypre -st_pc_hypre_type boomeramg -st_pc_hypre_boomeramg_agg_nl 1 -st_pc_hypre_boomeramg_P_max 4 -st_pc_hypre_boomeramg_interp_type standard -showtimes -eps_view
}}}

For a high quality preconditioner, such as boomeramg in this case, using PCG on the top of preconditioning is an overkill - the increase in complexity needed to setup the PCG solver does not lead to enough convergence acceleration. This is reversed if the preconditioner is cheap and poor. The direct application of Jacobi below makes lobpcg to converge in ~350 iterations:

{{{
 ./ex19 -da_grid_x 100 -da_grid_y 100 -da_grid_z 100 -eps_type blopex -eps_max_it 1000 -st_pc_type jacobi  -showtimes -eps_view
}}}

while the use of Jacobi-preconditioned PCG with 7 inner iterations makes lobpcg to converge in ~50 iterations:

{{{
./ex19 -da_grid_x 100 -da_grid_y 100 -da_grid_z 100 -eps_type blopex -st_ksp_type cg -st_pc_type jacobi -st_ksp_max_it 7  -showtimes -eps_view
}}}

The total number of inner-outer iterations is the same ~350 in both runs, but the latter is almost 3 times faster in CPU time, since a single PCG iteration is cheaper than a single LOBPCG iteration. 

PETSc command-line options supported, e.g., "-log_summary"