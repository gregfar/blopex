#summary PETSc install with AMD's OPEN64 compiler in Fedora 13: errors

= Introduction =

How To install PETSc with AMD's OPEN64 compiler in Fedora13 64 bit
(assuming bash shell)

= Preliminaries =

The basic system info:

{{{
>> uname -a
Linux opt4 2.6.33.5-124.fc13.x86_64 #1 SMP Fri Jun 11 09:38:12 UTC 2010 x86_64 x86_64 x86_64 GNU/Linux
}}}

Download from http://developer.amd.com/cpu/open64/ and install the AMD's OPEN64 compiler. 
The default location is /opt/x86_open64-4.2.3.2/ (depending on the version number). Make sure that you add the OPEN64 binaries location to your PATH: 

{{{
>> PATH=/opt/x86_open64-4.2.3.2/bin/:$PATH; export PATH
>>  which opencc
/opt/x86_open64-4.2.3.2/bin/opencc
}}}

Download and install ACML libraries, compiled with OPEN64: 
 * http://developer.amd.com/Downloads/acml-4-4-0-open64-64bit.tgz
 * http://developer.amd.com/Downloads/acml-4-4-0-open64-64bit-int64.tgz
The default locations are: 
   * /opt/acml-4-4-0-open64-64bit
   * /opt/acml-4-4-0-open64-64bit-int64

Each library comes in two different flavours, regular and mp, e.g., 
   * /opt/acml4.4.0/open64_64/lib
   * /opt/acml4.4.0/open64_64_mp/lib

According to Ch. 2 of the ACML manual, see  
http://developer.amd.com/assets/acml_userguide.pdf
one should use the mp version on a multi-CPU/core nodes.However, using the mp libraries requires having pre-installed MPI libraries (compiled with OPEN64 ?), which complicates the PETSc configure, which also asks for an MPI setup. I (AK) guess that a clean setup using the mp version of ACML and PETSc with MPI compiled using OPEN64 basically requires having an MPI library (either ONEMMPI or MPICH2) pre-compiled with OPEN64 and pre-installed, so that it could be used by both the mp version of ACML and PETSc with MPI. In this test, I do the simple combination, where a non-mp version of ACML is used and PETSc downloads and complies (with OPEN64) the MPI.


We need set the path to the libraries by running

{{{
>> LD_LIBRARY_PATH=/opt/acml4.4.0/open64_64/lib; export LD_LIBRARY_PATH
}}}

If SELinux is active, disable SELinux temporarily by running (as root) 

{{{
>> echo 0 > /selinux/enforce 
}}}

After you are done running the codes, restore SELinux by 

{{{
>> echo 1 > /selinux/enforce
}}}

Download and extract the latest PETSc, at the moment this is 
ftp://info.mcs.anl.gov/pub/petsc/release-snapshots/petsc-lite-3.1.tar.gz

Extract using *tar xzvf* , rename the PETSc dir as you like, cd to PETSc dir and run 

{{{
>> PETSC_DIR=$PWD; export PETSC_DIR
}}}

Version petsc-3.1-p3 and later already includes the latest BLOPEX code!

== PETSc configure without MPI ==

First you may (a must in petsc-3.1-p3) need to edit the file config/PETSc/packages/blopex.py and remove all instances of self.mpi there. BLOPEX does not require any MPI by itself. 

Second, 

{{{
>> ./configure --with-blas-lapack-dir=/opt/acml4.4.0/open64_64 --with-cc=opencc --with-cxx=openCC --with-fc=openf95 --download-blopex=1 --with-mpi=0 
>> make all
>> make test 
}}}

The "make all" may display some errors (actually warnings) in PETSc which can be ignored. 
the rest of the installation of BLOPEX (compiling the drivers) is standard. 


== PETSc configure with MPI ==

Run configure, e.g., 

{{{
>> ./configure --with-blas-lapack-dir=/opt/acml4.4.0/open64_64 --with-cc=opencc --with-cxx=openCC --with-fc=openf95 --download-blopex=1 --download-hypre=1 --download-openmpi=1
}}}

Here, we 
  * use the AMD's ACML library /opt/acml4.4.0/open64_64 for BLAS/LAPACK
  * specify the AMD's OPEN64 compilers, which are already in our PATH by options --with-cc=opencc --with-cxx=openCC --with-fc=openf95 
  * since this is the real arithmetic and 32-bit integers, we compile blopex and hypre together
  * we must specify the --download-openmpi=1 or --download-mpich=1 option, since the MPI libraries must be compiled with the same compiler as PETSc. This option not only compiles the MPI libraries, but also creates the local mpicc wrapper to opencc, etc, which is used during the make

Now, we run 
{{{
>> make PETSC_ARCH=linux-gnu-c-debug all
}}}
and hope that there are no errors. Now, the --download-openmpi=1 or --download-mpich=1 option also creates the local version of mpirun, which must be used instead whatever other mpirun binary you might have pre-installed on your system. So we add it in front of the PATH, in my case (it will also depend on the specified PETSC_ARCH)

{{{
>> PATH=/home/aknyazev/work/petsc/petsc-3.1-p3-OPENMPI_OPEN64_hypre/linux-gnu-c-debug/bin:$PATH; export PATH
>>  which mpirun
~/work/petsc/petsc-3.1-p3-OPENMPI_OPEN64_hypre/linux-gnu-c-debug/bin/mpirun
}}}

We also need to add the newly compiled local version of MPI libs to LD_LIBRARY_PATH by 
{{{
>> LD_LIBRARY_PATH=/home/aknyazev/work/petsc/petsc-3.1-p3-OPENMPI_OPEN64_hypre/linux-gnu-c-debug/lib:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH
}}}

 
{{{
>> make PETSC_ARCH=linux-gnu-c-debug test
}}}
passes OK. 

But trying to compile the BLOPEX driver gives an error: 
{{{
>> make driver
/home/aknyazev/work/petsc/petsc-3.1-p3-OPENMPI_OPEN64_hypre/linux-gnu-c-debug/bin/mpicc -o driver.o -c -g -I/home/aknyazev/work/petsc/petsc-3.1-p3-OPENMPI_OPEN64_hypre/linux-gnu-c-debug/include -I/home/aknyazev/work/petsc/petsc-3.1-p3-OPENMPI_OPEN64_hypre/linux-gnu-c-debug/include -I/home/aknyazev/work/petsc/petsc-3.1-p3-OPENMPI_OPEN64_hypre/include -I/home/aknyazev/work/petsc/petsc-3.1-p3-OPENMPI_OPEN64_hypre/linux-gnu-c-debug/include -D__INSDIR__=src/contrib/blopex/driver/ driver.c
/home/aknyazev/work/petsc/petsc-3.1-p3-OPENMPI_OPEN64_hypre/linux-gnu-c-debug/bin/mpicc -g  -o driver driver.o  -Wl,-rpath,/home/aknyazev/work/petsc/petsc-3.1-p3-OPENMPI_OPEN64_hypre/linux-gnu-c-debug/lib -L/home/aknyazev/work/petsc/petsc-3.1-p3-OPENMPI_OPEN64_hypre/linux-gnu-c-debug/lib -lpetsc   -lX11 -Wl,-rpath,/home/aknyazev/work/petsc/petsc-3.1-p3-OPENMPI_OPEN64_hypre/linux-gnu-c-debug/lib -L/home/aknyazev/work/petsc/petsc-3.1-p3-OPENMPI_OPEN64_hypre/linux-gnu-c-debug/lib -lBLOPEX -lHYPRE -lmpi_cxx -Wl,-rpath,/opt/x86_open64-4.2.3.2/lib/gcc-lib/x86_64-open64-linux/4.2.3.2 -Wl,-rpath,/opt/x86_open64-4.2.3.2/open64-gcc-4.2.0/lib/gcc/x86_64-redhat-linux/4.2.0 -Wl,-rpath,/opt/x86_open64-4.2.3.2/open64-gcc-4.2.0/lib/gcc -Wl,-rpath,/opt/x86_open64-4.2.3.2/open64-gcc-4.2.0/lib64 -Wl,-rpath,/opt/x86_open64-4.2.3.2/open64-gcc-4.2.0/lib -lstdc++ -Wl,-rpath,/opt/acml4.4.0/open64_64/lib -L/opt/acml4.4.0/open64_64/lib -lacml -lnsl -lrt -L/home/aknyazev/work/petsc/petsc-3.1-p3-OPENMPI_OPEN64_hypre/linux-gnu-c-debug/lib -L/opt/x86_open64-4.2.3.2/lib/gcc-lib/x86_64-open64-linux/4.2.3.2 -L/opt/x86_open64-4.2.3.2/open64-gcc-4.2.0/lib/gcc/x86_64-redhat-linux/4.2.0 -L/opt/x86_open64-4.2.3.2/open64-gcc-4.2.0/lib/gcc -L/opt/x86_open64-4.2.3.2/open64-gcc-4.2.0/lib64 -L/opt/x86_open64-4.2.3.2/open64-gcc-4.2.0/lib -ldl -lmpi -lopen-rte -lopen-pal -lnsl -lutil -lacml_mv -lmv -lpthread -lopen64rt -lgcc_s -lmpi_f90 -lmpi_f77 -lm -lm -lfortran -lm -lm -lffio -lm -lm -Wl,-rpath,/opt/x86_open64-4.2.3.2/lib/gcc-lib/x86_64-open64-linux/4.2.3.2 -Wl,-rpath,/opt/x86_open64-4.2.3.2/open64-gcc-4.2.0/lib/gcc/x86_64-redhat-linux/4.2.0 -Wl,-rpath,/opt/x86_open64-4.2.3.2/open64-gcc-4.2.0/lib/gcc -Wl,-rpath,/opt/x86_open64-4.2.3.2/open64-gcc-4.2.0/lib64 -Wl,-rpath,/opt/x86_open64-4.2.3.2/open64-gcc-4.2.0/lib -lm -lm -lm -lm -lm -lm -lmpi_cxx -lstdc++ -lmpi_cxx -lstdc++ -ldl -lmpi -lopen-rte -lopen-pal -lnsl -lutil -lacml_mv -lmv -lpthread -lopen64rt -lgcc_s -ldl 
driver.o: In function `main':
opt4.(none):/home/aknyazev/work/petsc/petsc-3.1-p3-OPENMPI_OPEN64_hypre/src/contrib/blopex/driver/driver.c:346: undefined reference to `lobpcg_solve_double'
collect2: ld returned 1 exit status
make: [driver] Error 1 (ignored)
/bin/rm -f driver.o
}}}